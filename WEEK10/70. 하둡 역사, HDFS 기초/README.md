# 하둡의 탄생

[4줄 요약]

1. 더그 커팅이라는 사람이 검색 인덱싱 라이브러리인 Lucene를 개발
2. Lucene를 기반으로 웹 검색엔진 프로젝트인 Nutch를 개발
3. Nutch를 사용할 때 불편했던 점을 해결하기 위해 Hadoop을 개발
4. 이후에도 Hadoop을 기반으로 한 여러 시스템들이 등장 (ecosystem)

[자세한 내용]

검색 페이지를 만드려면 [데이터 크롤링] → [데이터 저장] → [인덱싱 작업]을 거침

전세계의 데이터를 수집해서 인덱싱하려면 굉장히 큰 데이터를 분산 병렬로 처리해야함

이때 구글에서 논문을 발표한 Google File System을 통해 Nutch Distributed File System (NDFS)을 개발

같은 해에 구글에서 MapReduce (분산 처리 알고리즘)를 기반으로 Nutch에 구현함

이후 NDFS와 MapReduce를 Nutch에서 분리하여 Hadoop이라는 프로젝트로 독립

[성능]

2008년에 뉴옥타임즈에서 EC2, S3, Hadoop을 사용하여 2TB의 신문 기사를 36시간 만에 모두 PDF로 변환함

하둡을 사용하지 않고, 일반 컴퓨터로 사용했으면 당시 컴퓨팅 성능으로 14년이 소요

이외에 여러 에코시스템도 구글의 영향을 받음

* 에코시스템: 대규모 데이터, 분산 처리를 다루기 때문에 새로운 문제점들을 해결하기 위해 만들어진 프로젝트들
(ex. 스프링 시큐리티, 스프링 JPA, …)

### 하둡의 필요성

하나의 하드웨어에 모든 데이터를 저장하기보다, 100개의 하드웨어로 나눠서 저장하면 데이터를 읽는 시간이 1/100으로 줄어들지 않을까..??

문제점 1) 하드웨어에 문제가 발생했을 때 → Replication으로 해결

문제점 2) 데이터를 취합하는 과정에서 문제 또는 추가 리소스 발생 → MapReduce로 해결

문제점 3) 하둡은 네트워크로 연결된 여러개의 하드웨어에 저장하는 방식이기 때문에 네트워크 에러도 해결해야함

→ 그래서 하둡은 분산 파일 시스템을 만들 때, 하드웨어 하나가 고장나도 데이터의 오류 없이 잘 작동하는게 목적

### HDFS의 장단점

[장점]

- 대용량 데이터 파일 저장
- 스트리밍 방식의 데이터 접근
    - 랜덤 엑세스가 아닌 순차적으로 읽어서 접근함 (대용량 데이터를 순차적 처리하는데 유용)
- 데이터 무결성 (데이터 추가, 데이터 읽기만 가능함. 데이터 수정 불가능)

[단점]

- HDFS를 많은 데이터를 처리하는데 초점이 맞춰져 있기 때문에 데이터 접근에 대한 응답 속도가 느림
    - 현재는 HBase를 통해 더 빠르게 접근이 가능함 (HBase = 랜덤 엑세스)
- 용량이 적은 여러 파일을 저장하는데 부적합함
    - 메타데이터를 보관하는 곳은 메모리에서 동작하는 namenode 하나뿐인데, 하나의 큰 데이터보다 여러 데이터에 대한 정보를 namenode에 기록하게 되면 메모리 문제가 발생
    - 파일 시스템에 존재할 수 있는 파일의 수도 namenode의 크기에 따라 달라짐
- HDFS 파일은 하나의 writer에 의해 데이터를 추가함 (→ 동시성 문제에서 자유로움)

### HDFS 저장 방식

파일을 block-size (chunk) 개념을 활용해서 크기만큼 파일들을 나눔 (기본값 128MB)

ex) 500MB 파일일 경우 → 128MB + 128MB + 128MB + 116MB로 파일을 저장함

cf) 데이터베이스 기초 수업에 디스크 파트에 나오던 공식들을 사용함

[블록으로 저장할 경우 장점]

- 파일의 크기가 디스크의 크기를 넘어서도 문제가 없어진다. (운영체제 가상메모리의 사용 이유중 하나와 같음)
- 블록이라는 단위로 공통화 시켜서 저장 관련 시스템을 단순하게 만들 수 있다 (에러 종류도 정해져있음)
- 장애 복구나 고가용성을 지원하는데 필요한 replication을 구현할 수 있다.

### 네임 노드, 데이터 노드

HDFS 클러스터는 네임 노드(Master)와 데이터 노드(Slave)로 나뉨

[네임 노드]

네임노드는 파일이 어느 블록으로 이루어졌는지, 특정 블록은 어느 데이터 노드에 있는지 등의 정보를 기록해둔 메타데이터를 관리함. 그렇기 때문에 네임 노드가 고장나면 파일 시스템에서 어떤 파일도 찾지 못함 (데이터 노드를 통해 재구성이 불가능함)

* 네임노드는 SPOF가 되기 때문에 보조 네임노드를 두어서 주기적으로 백업하여 고가용성을 보장함

[데이터 노드]

데이터 노드는 요청이 들어오면 블록을 저장하고, 탐색하고, 블록의 목록을 주기적으로 네임 노드에 전달함

네임 노드에게 3초마다 Heartbeat를 보냄. 이때 얼마나 데이터를 적재할 수 있는지 관련 내용들이 있음

네임 노드는 데이터 노드에게 하트비트를 일정 시간 이상 못 받으면 해당 노드는 장애가 일어났다고 판단하고, 서비스 대상에서 제외함

[고가용성 보장]

네임노드가 SPOF이기 때문에 관련 처리를 통해 고가용성을 보장함

- 하둡 1.x 버전) 백업 네임 노드를 사용 → 한 번 고장날 때마다 복구하는데 30분의 시간이 걸림
- 하둡 2.x 버전) Active-Standby 모드를 사용하여 이중화를 함
    - Standby 네임 노드는 데이터만 전달 받고, 실제 작업을 처리하지 않음
    - Active 네임 노드가 고장나면 Standby 네임 노드가 바로 Active로 전환되어 수십초 안에 정상화 가능

[데이터 무결성 보장]

데이터 노드는 데이터 쓰기, 읽기를 할 때, 블록마다 checksum을 사용하여 검사한다.

[장애 허용 - Fault Tolerant]

하둡 2.x 버전에서는 Replication Factor를 사용함 (블록을 여러 데이터 노드에 복제)

만약 데이터 오류가 있으면 복제한 데이터를 통해 가져온다. (기본값은 3임)

Replication Factor = 3이면 두 번의 장애가 발생해도 문제 없음

저장 규칙은 첫번째와 두번째는 같은 랙에 저장, 세번째는 다른 랙에 저장

* Rack :데이터 센터에 있는 여러 대의 컴퓨터를 그룹화한 단위

### MapReduce

대용량 파일을 데이터 블록으로 나누고, 나중에 다시 블록을 합쳐서 파일로 만드는데 많은 리소스가 필요함

이때 사용하는 병렬 분산 처리 알고리즘

### HBase

![image](https://github.com/kauKoala/Backend-Study/assets/79046106/679c9172-2fbb-460a-978f-b094b961ffb0)

일반 RDB 기반와는 다른 컬럼 기반의 분산 NoSQL

특징은 데이터 분석시 avg(age)와 같은 쿼리문을 자주 날리는데, 쓸모 없는 정보를 읽지 않아도 되어서 빅데이터 분석시 빠른 속도를 낼 수 있음

### 출처

- 유튜브) T아카데미 - 아파치 하둡 입문
- 책) 하둡 완벽 가이드 (4판)
- 인프런) 빅데이터 파이프라인 마스터

### 번외

HDFS 헬스 체크

NOTICE.txt 파일을 /user/hadoop, /user/ubuntu에 각각 NOTICE.txt, input으로 2개 저장해둠

![image](https://github.com/kauKoala/Backend-Study/assets/79046106/c6f44744-fdee-4e07-addc-e9cf3ba630e1)

한 파일당 1개씩 총 2개의 블록이 생성된 것을 확인할 수 있음
